# Story 2: Article-Level Lifecycle Tracking

**Status**: Done
**Estimated Effort**: 3 hours
**Priority**: High
**Dependencies**: None (can run parallel with Story 1)

## User Story

As a **developer investigating why specific articles failed or went missing**,
I want **to track each article's journey from extraction through persistence with a unique ID**,
So that **I can trace any article's complete lifecycle in the logs**.

## Acceptance Criteria

1. ✅ Each article gets unique tracking ID when queued for extraction
2. ✅ Tracking ID appears in all logs related to that article
3. ✅ Lifecycle stages logged: queued, extracted, validated, persisted, skipped, failed
4. ✅ Logs include source attribution (name, ID, URL) at each stage
5. ✅ Can search logs by tracking ID to see article's full journey
6. ✅ Overhead <10ms per article

## Technical Approach

### Files to Modify
- `services/scraper/src/enhanced-scraper.ts`
- `services/scraper/src/enhanced-logger.ts`
- `services/scraper/src/types.ts`

### Implementation Steps

1. **Generate tracking IDs** (`enhanced-scraper.ts`):
   - Add `articleTrackingId` to crawler `userData`
   - Generate UUID for each article request
   - Pass through entire pipeline

2. **Add lifecycle logging method** (`enhanced-logger.ts`):
   - New method: `logArticleLifecycle()`
   - Accepts stage and details
   - Logs with consistent event structure

3. **Integrate logging at each stage**:
   - **Queue**: When article added to crawler requests
   - **Extraction**: After content successfully extracted
   - **Persistence Success**: After database INSERT returns
   - **Persistence Skipped**: When duplicate detected
   - **Persistence Failed**: When INSERT errors

4. **Add article tracking to persistence** (`enhanced-scraper.ts`):
   - Preserve tracking ID through articleData object
   - Log before and after each INSERT operation

### Code Example

```typescript
// In scrapeSourceEnhanced, creating article requests
const articleRequests = candidateItems.slice(0, targetCandidates).map(item => {
  const articleTrackingId = crypto.randomUUID();

  // Log article queued
  this.logger.logArticleLifecycle(
    jobId, source.id, articleTrackingId, 'queued',
    { sourceName: source.name, sourceUrl: item.link! }
  );

  return {
    url: item.link!,
    userData: {
      jobId,
      sourceId: source.id,
      sourceName: source.name,
      articleTitle: item.title,
      articleTrackingId,
      correlationId: this.logger.createCorrelationId()
    }
  };
});

// In request handler, after successful extraction
await this.logger.logArticleLifecycle(
  jobId, sourceId, articleTrackingId, 'extracted',
  {
    sourceName,
    sourceUrl: request.url,
    contentLength: article.content.length,
    qualityScore: this.logger.calculateQualityScore(article)
  }
);

// Store tracking ID in article data
const articleData = {
  // ... existing fields
  articleTrackingId,  // Add this
};

// In saveArticlesTransactionally, after INSERT success
await this.logger.logArticleLifecycle(
  jobId, sourceResult.sourceId, article.articleTrackingId, 'persisted',
  {
    sourceName: sourceResult.sourceName,
    sourceUrl: article.sourceUrl,
    databaseArticleId: result.rows[0].id
  }
);

// When duplicate detected
await this.logger.logArticleLifecycle(
  jobId, sourceResult.sourceId, article.articleTrackingId, 'skipped',
  {
    sourceName: sourceResult.sourceName,
    sourceUrl: article.sourceUrl,
    reason: 'duplicate_url_or_content_hash'
  }
);
```

## Testing Steps

### Unit Tests
1. Test tracking ID generation is unique
2. Test `logArticleLifecycle()` creates correct log structure
3. Test tracking ID persists through pipeline

### Integration Tests
1. Run scraping job
2. Select random tracking ID from logs
3. Search all logs for that ID
4. Verify can see complete journey: queued → extracted → persisted

### Manual Testing
1. Run scraping job with 10 articles
2. Pick one article URL
3. Search logs for that URL
4. Find its tracking ID
5. Search logs by tracking ID
6. Verify see all lifecycle stages
7. Verify final stage shows database article ID

### Expected Log Sequence

```json
// Stage 1: Queued
{
  "event_type": "article_lifecycle",
  "event_name": "article_queued",
  "article_tracking_id": "track-uuid-123",
  "source_name": "CNN",
  "source_url": "https://cnn.com/article",
  "stage": "queued"
}

// Stage 2: Extracted
{
  "event_type": "article_lifecycle",
  "event_name": "article_extracted",
  "article_tracking_id": "track-uuid-123",
  "source_name": "CNN",
  "source_url": "https://cnn.com/article",
  "stage": "extracted",
  "content_length": 4521,
  "quality_score": 90
}

// Stage 3: Persisted
{
  "event_type": "article_lifecycle",
  "event_name": "article_persisted",
  "article_tracking_id": "track-uuid-123",
  "source_name": "CNN",
  "source_url": "https://cnn.com/article",
  "stage": "persisted",
  "database_article_id": "db-uuid-456"
}
```

## Definition of Done

- [ ] Code implemented with tracking ID generation
- [ ] All lifecycle stages log with tracking ID
- [ ] Can query logs by tracking ID
- [ ] Unit tests verify tracking ID propagation
- [ ] Integration test traces article end-to-end
- [ ] Performance overhead measured <10ms per article
- [ ] Documentation updated with tracking ID usage
- [ ] Code reviewed and approved
- [ ] Merged to main branch

## Notes

- Tracking ID is separate from correlation ID (which tracks HTTP requests)
- Tracking ID survives across retries (correlation ID doesn't)
- Enables debugging: "Why didn't article X get saved?"
- Foundation for future audit trails and compliance
- Consider adding tracking ID to database schema in future (out of scope for this story)
