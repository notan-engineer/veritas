# 2. Enhanced Logging & Monitoring - New

## User Story
As a **system administrator**, I want to **see efficiency metrics and capping details in logs and UI** so that **I can monitor scraper performance and identify opportunities for optimization**.

## Acceptance Criteria
- [ ] Given a completed scraping job, when I view the job logs, then I see clear metrics for extracted vs. saved vs. capped articles
- [ ] Given a job where capping occurred, when I view the scraper dashboard, then I see a visual indicator showing efficiency percentage
- [ ] Given multiple jobs over time, when I review metrics, then I can identify patterns of resource waste
- [ ] Given job completion logs, when I analyze them, then I see aggregate efficiency metrics across all sources

## Technical Approach

### Database Changes
**None required** - Uses data from Story 1.

### API Changes
**None required** - May add convenience endpoint for metrics aggregation (optional).

### Code Changes

#### File: `services/scraper/src/enhanced-logger.ts`

**Add new logging method for efficiency metrics:**

```typescript
/**
 * Log job completion with efficiency metrics
 */
async logJobCompletionWithEfficiency(
  jobId: string,
  metrics: EnhancedJobMetrics,
  totalExtracted: number,
  totalSaved: number,
  totalCapped: number,
  durationMs: number
): Promise<void> {
  const efficiency = totalExtracted > 0
    ? Math.round((totalSaved / totalExtracted) * 100)
    : 100;

  const wastePercentage = totalExtracted > 0
    ? Math.round((totalCapped / totalExtracted) * 100)
    : 0;

  await logJobActivity({
    jobId,
    level: 'info',
    message: `Job completed: ${totalSaved}/${metrics.totals.targetArticles} articles saved in ${(durationMs / 1000).toFixed(2)}s (${efficiency}% efficiency)`,
    additionalData: {
      event_type: 'lifecycle',
      event_name: 'job_completed_with_efficiency',
      timestamp_ms: Date.now(),
      duration_seconds: Math.round(durationMs / 1000),
      efficiency_metrics: {
        total_extracted: totalExtracted,
        total_saved: totalSaved,
        total_capped: totalCapped,
        efficiency_percentage: efficiency,
        waste_percentage: wastePercentage,
        target_articles: metrics.totals.targetArticles
      },
      enhanced_metrics: metrics
    }
  });
}
```

#### File: `services/scraper/src/enhanced-scraper.ts`

**Update `logFinalJobMetrics()` method (line ~609):**

```typescript
private async logFinalJobMetrics(
  jobId: string,
  successfulExtractions: SourceResult[],
  persistenceResults: Map<string, SourcePersistenceResult>,
  extractionFailures: Record<string, string>,
  articlesPerSource: number,
  startTime: number
): Promise<void> {
  // Build enhanced metrics
  const metrics: EnhancedJobMetrics = {
    sources: {},
    totals: {
      targetArticles: 0,
      candidatesProcessed: 0,
      extracted: 0,
      saved: 0,
      duplicates: 0,
      capped: 0, // NEW: Add capped tracking
      actualSuccessRate: 0,
      efficiencyPercentage: 0 // NEW: Add efficiency
    }
  };

  // Add extraction failures
  for (const [sourceName, error] of Object.entries(extractionFailures)) {
    metrics.sources[sourceName] = {
      extracted: 0,
      saved: 0,
      duplicates: 0,
      failures: 0,
      capped: 0, // NEW
      success: false
    };
  }

  // Add successful extractions and persistence results
  let totalCapped = 0;
  for (const extraction of successfulExtractions) {
    const persistence = persistenceResults.get(extraction.sourceName);
    const cappedCount = persistence?.cappedCount || 0;
    totalCapped += cappedCount;

    metrics.sources[extraction.sourceName] = {
      extracted: extraction.extractedArticles.length,
      saved: persistence?.savedCount || 0,
      duplicates: persistence?.duplicatesSkipped || 0,
      failures: persistence?.saveFailures || 0,
      capped: cappedCount, // NEW
      success: (persistence?.savedCount || 0) > 0
    };

    metrics.totals.candidatesProcessed += extraction.extractionMetrics.candidatesProcessed;
    metrics.totals.extracted += extraction.extractedArticles.length;
    metrics.totals.saved += persistence?.savedCount || 0;
    metrics.totals.duplicates += persistence?.duplicatesSkipped || 0;
  }

  const totalSources = Object.keys(metrics.sources).length;
  metrics.totals.targetArticles = totalSources * articlesPerSource;
  metrics.totals.capped = totalCapped; // NEW
  metrics.totals.actualSuccessRate = metrics.totals.targetArticles > 0
    ? metrics.totals.saved / metrics.totals.targetArticles
    : 0;
  metrics.totals.efficiencyPercentage = metrics.totals.extracted > 0
    ? Math.round((metrics.totals.saved / metrics.totals.extracted) * 100)
    : 100; // NEW

  // Use new logging method with efficiency metrics
  await this.logger.logJobCompletionWithEfficiency(
    jobId,
    metrics,
    metrics.totals.extracted,
    metrics.totals.saved,
    metrics.totals.capped,
    Date.now() - startTime
  );
}
```

#### File: `services/scraper/src/types.ts`

**Update interfaces to include efficiency metrics:**

```typescript
export interface EnhancedJobMetrics {
  sources: Record<string, {
    extracted: number;
    saved: number;
    duplicates: number;
    failures: number;
    capped?: number; // NEW
    success: boolean;
  }>;
  totals: {
    targetArticles: number;
    candidatesProcessed: number;
    extracted: number;
    saved: number;
    duplicates: number;
    capped?: number; // NEW
    actualSuccessRate: number;
    efficiencyPercentage?: number; // NEW
  };
}
```

### UI Changes

#### File: `services/ui/app/scraper/page.tsx`

**Add efficiency display to job completion UI:**

```typescript
// In the job display section, add efficiency badge
{job.status === 'successful' && job.efficiency_percentage && (
  <Badge variant={job.efficiency_percentage >= 80 ? "default" : "warning"}>
    {job.efficiency_percentage}% efficient
  </Badge>
)}

{job.total_capped > 0 && (
  <Badge variant="secondary">
    {job.total_capped} capped
  </Badge>
)}
```

#### File: `services/ui/app/scraper/components/JobMetrics.tsx` (NEW)

**Create new component for detailed metrics:**

```typescript
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";
import { Badge } from "@/components/ui/badge";

interface JobMetricsProps {
  extracted: number;
  saved: number;
  capped: number;
  target: number;
}

export function JobMetrics({ extracted, saved, capped, target }: JobMetricsProps) {
  const efficiency = extracted > 0 ? Math.round((saved / extracted) * 100) : 100;
  const waste = extracted > 0 ? Math.round((capped / extracted) * 100) : 0;
  const targetMet = Math.round((saved / target) * 100);

  return (
    <Card>
      <CardHeader>
        <CardTitle>Performance Metrics</CardTitle>
        <CardDescription>Extraction efficiency and resource usage</CardDescription>
      </CardHeader>
      <CardContent className="space-y-2">
        <div className="flex justify-between">
          <span className="text-sm text-muted-foreground">Extracted:</span>
          <span className="font-medium">{extracted}</span>
        </div>
        <div className="flex justify-between">
          <span className="text-sm text-muted-foreground">Saved:</span>
          <span className="font-medium">{saved}</span>
        </div>
        {capped > 0 && (
          <div className="flex justify-between">
            <span className="text-sm text-muted-foreground">Capped:</span>
            <Badge variant="secondary">{capped}</Badge>
          </div>
        )}
        <div className="flex justify-between pt-2 border-t">
          <span className="text-sm text-muted-foreground">Efficiency:</span>
          <Badge variant={efficiency >= 80 ? "default" : "warning"}>
            {efficiency}%
          </Badge>
        </div>
        {waste > 0 && (
          <div className="flex justify-between">
            <span className="text-sm text-muted-foreground">Resource Waste:</span>
            <Badge variant={waste > 30 ? "destructive" : "secondary"}>
              {waste}%
            </Badge>
          </div>
        )}
        <div className="flex justify-between">
          <span className="text-sm text-muted-foreground">Target Met:</span>
          <Badge variant={targetMet >= 90 ? "default" : "warning"}>
            {targetMet}%
          </Badge>
        </div>
      </CardContent>
    </Card>
  );
}
```

### Implementation Context
```
Required files:
- @services/scraper/src/enhanced-logger.ts (new method)
- @services/scraper/src/enhanced-scraper.ts (update logFinalJobMetrics)
- @services/scraper/src/types.ts (update interfaces)
- @services/ui/app/scraper/page.tsx (add efficiency badges)
- @services/ui/app/scraper/components/JobMetrics.tsx (NEW component)
```

## Success Test

### Testing Steps

1. **Complete Story 1 implementation first**

2. **Trigger test job:**
```bash
# Via UI at http://localhost:3000/scraper
# Or via API:
curl -X POST http://localhost:3001/api/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "sources": ["BBC News", "CNN"],
    "articlesPerSource": 10
  }'
```

3. **Check enhanced logs:**
```bash
node utilities/06-test-logs.js <job_id>
# Look for "job_completed_with_efficiency" event
```

**Expected log output:**
```json
{
  "level": "info",
  "message": "Job completed: 18/20 articles saved in 5.23s (90% efficiency)",
  "additionalData": {
    "event_name": "job_completed_with_efficiency",
    "efficiency_metrics": {
      "total_extracted": 20,
      "total_saved": 18,
      "total_capped": 2,
      "efficiency_percentage": 90,
      "waste_percentage": 10,
      "target_articles": 20
    }
  }
}
```

4. **Verify UI displays metrics:**
   - Navigate to scraper dashboard
   - View completed job
   - Check for efficiency badge
   - Verify "capped" indicator if applicable

5. **Test edge cases:**
   - Job with 100% efficiency (no capping)
   - Job with high waste (> 30% capped)
   - Job with multiple sources (mixed efficiency)

### Testing Utilities
- [x] `utilities/06-test-logs.js` - Verify enhanced logging
- [ ] Manual UI testing in scraper dashboard

### Validation Checklist
- [ ] Logs show efficiency percentage
- [ ] Logs show waste percentage when capping occurs
- [ ] UI displays efficiency badge with appropriate color
- [ ] UI shows capped count when > 0
- [ ] JobMetrics component renders correctly
- [ ] Metrics are accurate (manual calculation verification)

## Dependencies
- Previous stories: **Story 1 must be complete** (provides `cappedCount` data)
- External dependencies: None (uses existing shadcn/ui components)

## Implementation Notes

### Color Coding Guidelines

**Efficiency Badge:**
- ≥80%: Green (`default` variant) - Good efficiency
- <80%: Yellow (`warning` variant) - Room for optimization

**Waste Badge:**
- ≤30%: Gray (`secondary` variant) - Acceptable
- >30%: Red (`destructive` variant) - High waste, consider optimization

**Target Met Badge:**
- ≥90%: Green (`default` variant) - Goal achieved
- <90%: Yellow (`warning` variant) - Underperformance

### Metrics Interpretation Guide

**Efficiency Percentage:**
- Formula: `(saved / extracted) * 100`
- Meaning: What % of extracted articles were actually saved
- Good: ≥80%
- Indicates: Capping overhead

**Waste Percentage:**
- Formula: `(capped / extracted) * 100`
- Meaning: What % of work was discarded
- Good: ≤30%
- Indicates: Opportunity for upstream optimization

**Target Met:**
- Formula: `(saved / target) * 100`
- Meaning: Did we achieve the article goal?
- Good: ≥90%
- Indicates: Source reliability and content availability

### UI Design Considerations

**Responsive Design:**
- Metrics card should work on mobile
- Use flexbox for layout
- Badge sizing appropriate for viewport

**Dark Mode Compatibility:**
- All components use shadcn/ui theme system
- Badges inherit theme colors
- Test in both light and dark modes

**Accessibility:**
- Badge variants have sufficient color contrast
- Screen reader labels for metrics
- Semantic HTML structure

## Progress Tracking
- [ ] Development started
- [ ] Enhanced logger method implemented
- [ ] Metrics calculation added to enhanced-scraper.ts
- [ ] Type interfaces updated
- [ ] UI component created (JobMetrics.tsx)
- [ ] Scraper dashboard updated with badges
- [ ] Local testing completed
- [ ] Code review completed
- [ ] Merged to feature branch
- [ ] Deployed to production
- [ ] Production validation completed
