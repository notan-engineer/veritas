# 1. Implement Post-Processing Article Limit - New

## User Story
As a **system administrator**, I want the scraper to **respect the requested articles-per-source limit** so that **I have predictable, controlled content collection without over-scraping**.

## Acceptance Criteria
- [ ] Given a scraping job requesting N articles per source, when more than N articles are successfully extracted, then exactly N articles are saved to the database
- [ ] Given a scraping job requesting 10 articles from CNN, when 12 articles are extracted, then exactly 10 articles are persisted and 2 are discarded
- [ ] Given multiple sources in a job with different extraction success rates, when the job completes, then each source independently respects its article limit
- [ ] Given articles are capped for a source, when the job completes, then a log entry documents the capping event with extracted count, saved count, and capped count

## Technical Approach

### Database Changes
**None required** - This is a code-only implementation.

### API Changes
**None required** - Internal logic change only.

### Code Changes

#### File: `services/scraper/src/enhanced-scraper.ts`

**Location:** Method `saveArticlesTransactionally()` (approximately line 461)

**Current Logic:**
```typescript
// Process articles by source for accurate tracking
for (const sourceResult of sourceResults) {
  // ... setup ...

  // Insert articles for this source
  for (const article of sourceResult.extractedArticles) {
    // ... save all extracted articles ...
  }
}
```

**New Logic:**
```typescript
// Process articles by source for accurate tracking
for (const sourceResult of sourceResults) {
  const sourcePersistence: SourcePersistenceResult = {
    sourceName: sourceResult.sourceName,
    sourceId: sourceResult.sourceId,
    savedCount: 0,
    duplicatesSkipped: 0,
    saveFailures: 0,
    cappedCount: 0, // NEW: Track capped articles
    articles: []
  };

  // NEW: Limit articles to target per source
  const articlesToSave = sourceResult.extractedArticles.slice(0, articlesPerSource);
  const cappedCount = sourceResult.extractedArticles.length - articlesToSave.length;

  if (cappedCount > 0) {
    sourcePersistence.cappedCount = cappedCount;
    totalCappedCount += cappedCount;

    // Log capping event
    await logJobActivity({
      jobId,
      sourceId: sourceResult.sourceId,
      level: 'info',
      message: `Capped ${cappedCount} excess articles for ${sourceResult.sourceName}`,
      additionalData: {
        event_type: 'persistence',
        event_name: 'articles_capped',
        target: articlesPerSource,
        extracted: sourceResult.extractedArticles.length,
        capped: cappedCount,
        timestamp_ms: Date.now()
      }
    });
  }

  // Insert ONLY the capped articles for this source
  for (const article of articlesToSave) {
    // ... existing save logic unchanged ...
  }

  persistenceResults.set(sourceResult.sourceName, sourcePersistence);
}
```

**Key Changes:**
1. Add `cappedCount` tracking variable
2. Slice extracted articles to target limit: `extractedArticles.slice(0, articlesPerSource)`
3. Calculate capped count: `extracted.length - articlesToSave.length`
4. Log capping event if `cappedCount > 0`
5. Iterate over `articlesToSave` instead of all `extractedArticles`

#### File: `services/scraper/src/types.ts`

**Location:** Interface `SourcePersistenceResult`

**Current Interface:**
```typescript
export interface SourcePersistenceResult {
  sourceName: string;
  sourceId: string;
  savedCount: number;
  duplicatesSkipped: number;
  saveFailures: number;
  articles: string[];
}
```

**New Interface:**
```typescript
export interface SourcePersistenceResult {
  sourceName: string;
  sourceId: string;
  savedCount: number;
  duplicatesSkipped: number;
  saveFailures: number;
  cappedCount?: number; // NEW: Track articles discarded due to limit
  articles: string[];
}
```

**Rationale:** Optional field maintains backward compatibility with existing code.

### Implementation Context
```
Required files:
- @services/scraper/src/enhanced-scraper.ts (line ~461-540)
- @services/scraper/src/types.ts (SourcePersistenceResult interface)
```

## Success Test

### Local Testing Steps

1. **Clean test database:**
```bash
cd utilities
node 02-db-clear.js --confirm
```

2. **Run scraper test:**
```bash
node 03-test-scraper.js
```

3. **Trigger a test job via UI:**
   - Navigate to http://localhost:3000/scraper
   - Select sources: BBC News, CNN, Reuters
   - Set articles per source: 10
   - Click "Start Scraping Job"

4. **Monitor logs:**
```bash
# Check scraper service logs
# Look for "articles_capped" events
```

5. **Verify database counts:**
```bash
# From project root
psql -h localhost -U local_user -d veritas_local -c "
  SELECT
    s.name as source,
    COUNT(*) as articles_saved,
    j.articles_per_source as requested
  FROM scraped_content sc
  JOIN sources s ON sc.source_id = s.id
  JOIN scraping_jobs j ON sc.job_id = j.id
  WHERE j.id = '<latest_job_id>'
  GROUP BY s.name, j.articles_per_source
  ORDER BY s.name;
"
```

**Expected Results:**
- CNN: 10 articles saved (capped from 12+)
- BBC News: ≤10 articles saved (may be less due to extraction failures)
- Reuters: 0 or fewer articles (may fail)
- Logs contain "Capped X excess articles for CNN" message

6. **Analyze job logs:**
```bash
node utilities/06-test-logs.js <job_id>
# Look for:
# - "articles_capped" events
# - Extraction vs. saved counts
```

### Testing Utilities
- [x] `utilities/02-db-clear.js` - Clean test database
- [x] `utilities/03-test-scraper.js` - End-to-end scraper test
- [x] `utilities/06-test-logs.js` - Analyze job logs

### Validation Checklist
- [ ] Database query shows ≤ requested articles per source
- [ ] Logs contain capping events with correct metrics
- [ ] No errors in scraper service
- [ ] Job completes with "successful" or "partial" status
- [ ] UI shows completed job with metrics

## Dependencies
- Previous stories: None (first story in project)
- External dependencies: None (uses existing libraries)

## Implementation Notes

### Design Decisions

**Why slice first N articles?**
- Simplest, most predictable behavior
- Preserves temporal order (first extracted = most recent in RSS)
- Deterministic (same input = same output)

**Why log at 'info' level?**
- This is expected behavior, not an error
- Provides transparency without alarming operators
- Can be filtered/aggregated for analysis

**Why optional `cappedCount` field?**
- Maintains backward compatibility
- Allows gradual rollout
- Doesn't break existing persistence logic

### Edge Cases Handled

1. **Exact match (extracted = target):**
   - `cappedCount = 0`
   - No capping log
   - All articles saved

2. **Under-extraction (extracted < target):**
   - `cappedCount = 0`
   - No capping log
   - All extracted articles saved

3. **Over-extraction (extracted > target):**
   - `cappedCount = extracted - target`
   - Capping log generated
   - Only first N articles saved

4. **Zero articles requested:**
   - Edge case: Shouldn't happen in practice
   - Behavior: `slice(0, 0)` returns empty array
   - Result: No articles saved (correct)

### Performance Considerations

**Array Slice Operation:**
- Time complexity: O(N) where N = articlesPerSource
- Space complexity: O(N) for new array
- Impact: Negligible (N typically 10-20)

**Logging Overhead:**
- Only logs when capping occurs
- Async operation (doesn't block)
- Impact: < 1ms per source

**Total Overhead:**
- Expected: < 1ms per source
- Acceptable: Function already does O(N) database inserts

### Testing Strategy

**Unit Test Scenarios:**
- Test with `articlesPerSource = 10`, extracted = 5 (under)
- Test with `articlesPerSource = 10`, extracted = 10 (exact)
- Test with `articlesPerSource = 10`, extracted = 15 (over)
- Test with `articlesPerSource = 1`, extracted = 20 (extreme cap)

**Integration Test:**
- Full scraping job with real sources
- Verify end-to-end behavior
- Check database consistency

**Regression Test:**
- Ensure existing jobs still work
- No change to extraction logic
- Same performance characteristics

## Progress Tracking
- [ ] Development started
- [ ] Code changes implemented in enhanced-scraper.ts
- [ ] Type interface updated in types.ts
- [ ] Local testing completed (all scenarios pass)
- [ ] Code review completed
- [ ] Merged to feature branch
- [ ] Deployed to production
- [ ] Production validation completed
