# Story 1: Database Verification Logging

**Status**: Done
**Estimated Effort**: 2 hours
**Priority**: High
**Dependencies**: None

## User Story

As a **developer debugging scraping discrepancies**,
I want **the system to automatically verify logged metrics against actual database state**,
So that **I can immediately detect when logs don't match reality**.

## Acceptance Criteria

1. ✅ After persistence completes, system queries database for actual saved counts per source
2. ✅ System compares database counts with logged persistence counts
3. ✅ Discrepancies are logged at WARNING level
4. ✅ Verification log includes sample article IDs from database
5. ✅ Verification query completes in <200ms
6. ✅ No impact on existing UI functionality

## Technical Approach

### Files to Modify
- `services/scraper/src/enhanced-scraper.ts`
- `services/scraper/src/enhanced-logger.ts`
- `services/scraper/src/types.ts`

### Implementation Steps

1. **Add verification query method** (`enhanced-scraper.ts`):
   - Query `scraped_content` grouped by `source_id` for specific `job_id`
   - Join with `sources` table to get source names
   - Return Map of source name → actual count
   - Include sample article IDs for spot-checking

2. **Add logging method** (`enhanced-logger.ts`):
   - New method: `logDatabaseVerification()`
   - Compare persistence results with database counts
   - Log at WARNING if discrepancies found
   - Include detailed breakdown per source

3. **Integrate into scrapeJob workflow**:
   - Call verification after `saveArticlesTransactionally()` completes
   - Pass persistence results and database counts to logger
   - Log before final job completion

4. **Add types** (`types.ts`):
   - `DatabaseVerificationResult` interface
   - Extend log event types

### Code Example

```typescript
// In enhanced-scraper.ts
private async verifyPersistenceResults(
  jobId: string,
  persistenceResults: Map<string, SourcePersistenceResult>
): Promise<Map<string, DatabaseVerificationResult>> {
  const { pool } = await import('./database');

  const result = await pool.query(`
    SELECT
      s.name as source_name,
      s.id as source_id,
      COUNT(sc.id) as actual_count,
      array_agg(sc.id ORDER BY sc.created_at LIMIT 5) as sample_ids
    FROM scraped_content sc
    JOIN sources s ON sc.source_id = s.id
    WHERE sc.job_id = $1
    GROUP BY s.name, s.id
    ORDER BY s.name
  `, [jobId]);

  return new Map(result.rows.map(row => [
    row.source_name,
    {
      sourceId: row.source_id,
      actualCount: parseInt(row.actual_count),
      sampleIds: row.sample_ids || []
    }
  ]));
}

// In scrapeJob method, after saveArticlesTransactionally
const verificationResults = await this.verifyPersistenceResults(jobId, persistenceResults);
await this.logger.logDatabaseVerification(jobId, persistenceResults, verificationResults);
```

## Testing Steps

### Unit Tests
1. Test `verifyPersistenceResults()` with known job_id
2. Test `logDatabaseVerification()` with matching counts
3. Test `logDatabaseVerification()` with discrepancies

### Integration Tests
1. Create test job with 3 sources
2. Save known number of articles per source
3. Run verification and assert correct counts logged

### Manual Testing
1. Run scraping job with BBC, CNN, Reuters
2. Check logs for verification entry
3. Manually query database to confirm counts match logs
4. Verify sample article IDs exist in database

### Expected Log Output

```json
{
  "event_type": "verification",
  "event_name": "database_verification_completed",
  "job_id": "...",
  "verification_results": {
    "BBC News": {
      "source_id": "...",
      "persistence_claimed": 8,
      "database_actual": 8,
      "discrepancy": false,
      "sample_article_ids": ["id1", "id2", "id3", "id4", "id5"]
    },
    "CNN": {
      "source_id": "...",
      "persistence_claimed": 12,
      "database_actual": 0,
      "discrepancy": true,
      "discrepancy_amount": -12,
      "sample_article_ids": []
    }
  },
  "total_claimed": 20,
  "total_actual": 8,
  "has_discrepancies": true
}
```

## Definition of Done

- [ ] Code implemented and passing tests
- [ ] Unit tests cover verification logic
- [ ] Integration test validates end-to-end flow
- [ ] Manual testing confirms logs appear correctly
- [ ] Performance verified (<200ms query time)
- [ ] UI can display new log events without errors
- [ ] Code reviewed and approved
- [ ] Merged to main branch

## Notes

- This is the foundation for all other logging enhancements
- Provides "ground truth" from database to compare against
- Query performance is critical - uses indexed columns (`job_id`, `source_id`)
- Sample IDs enable spot-checking specific articles
