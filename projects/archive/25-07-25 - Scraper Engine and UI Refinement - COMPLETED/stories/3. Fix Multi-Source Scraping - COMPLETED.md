# User Story 3: Reliably Scrape and Persist Content from Multiple Sources

**Status**: âœ… COMPLETED  
**Epic**: Streamline Core Scraping Functionality & Management  
**Priority**: Critical  
**Estimated Effort**: 6 hours

## User Story

As a **content manager**, I want **all requested articles from all sources to be saved** so that I can **trust the scraper to gather comprehensive content without manual verification**.

## Background

The current implementation has a critical bug where multi-source scraping jobs fail to persist all content. The concurrent task aggregation logic appears broken, causing some or all articles to be lost despite the job showing as "completed". This is the most critical issue affecting system reliability.

## Acceptance Criteria

- [ ] Multi-source jobs save ALL requested content
- [ ] Each source's success/failure is tracked independently  
- [ ] Failed sources don't prevent other sources from saving
- [ ] Job status accurately reflects what was scraped
- [ ] Database transactions ensure data consistency
- [ ] Content count in scraped_content matches requested amount

## Technical Approach

### 1. Fix Concurrent Result Aggregation
```typescript
// services/scraper/src/scraper.ts
// Current logic likely loses results from Promise.all or similar

// Better approach:
const results = await Promise.allSettled(
  sources.map(source => scrapeSource(source, articlesPerSource))
);

const allArticles = [];
const sourceResults = {};

results.forEach((result, index) => {
  const source = sources[index];
  if (result.status === 'fulfilled') {
    allArticles.push(...result.value.articles);
    sourceResults[source.id] = { 
      success: true, 
      count: result.value.articles.length 
    };
  } else {
    console.error(`Source ${source.id} failed:`, result.reason);
    sourceResults[source.id] = { 
      success: false, 
      error: result.reason.message 
    };
  }
});
```

### 2. Implement Transactional Persistence
```typescript
// Wrap in transaction to ensure all-or-nothing
await db.transaction(async (trx) => {
  // Insert all articles
  if (allArticles.length > 0) {
    await trx('scraped_content').insert(allArticles);
  }
  
  // Update job with accurate status
  const successCount = Object.values(sourceResults)
    .filter(r => r.success).length;
  
  let finalStatus;
  if (successCount === sources.length) {
    finalStatus = 'successful';
  } else if (successCount > 0) {
    finalStatus = 'partial';
  } else {
    finalStatus = 'failed';
  }
  
  await trx('scraping_jobs')
    .where({ id: jobId })
    .update({ 
      status: finalStatus,
      completed_at: new Date(),
      metadata: { sourceResults }
    });
});
```

### 3. Add Detailed Logging
```typescript
// Log at each step for debugging
await logToDb(jobId, 'info', `Starting scrape for ${sources.length} sources`);

// Per-source logging
await logToDb(jobId, 'info', `Source ${source.id}: Found ${articles.length} articles`);

// Final summary
await logToDb(jobId, 'info', `Job complete: ${allArticles.length} total articles saved`);
```

## Implementation Steps

1. **Analyze current implementation**
   - Find exact location of aggregation bug
   - Understand current flow
   - Identify where articles are lost

2. **Refactor aggregation logic**
   - Use Promise.allSettled for fault tolerance
   - Collect all results properly
   - Track per-source outcomes

3. **Implement transaction wrapper**
   - Begin transaction before writes
   - Insert all content
   - Update job status
   - Commit or rollback together

4. **Add comprehensive logging**
   - Log start of each source
   - Log article counts
   - Log any errors
   - Log final summary

5. **Test with multiple scenarios**
   - All sources succeed
   - Some sources fail
   - All sources fail
   - Network interruptions

## Test Scenarios

### Scenario 1: Happy Path
1. Configure 3 sources with valid RSS feeds
2. Request 5 articles per source
3. Run scraping job
4. Verify exactly 15 articles in scraped_content
5. Verify job status is "successful"

### Scenario 2: Partial Failure
1. Configure 3 sources (1 with invalid URL)
2. Request 5 articles per source
3. Run scraping job
4. Verify exactly 10 articles saved (from 2 good sources)
5. Verify job status is "partial"
6. Check logs show which source failed

### Scenario 3: Transaction Integrity
1. Configure sources that will succeed
2. Simulate database error during job update
3. Verify NO articles were saved (transaction rolled back)
4. Verify job status unchanged

## Verification Steps

### Pre-Implementation
- [ ] Locate exact aggregation bug in code
- [ ] Understand current database write pattern
- [ ] Check if transactions are used currently
- [ ] Review existing error handling

### During Implementation
- [ ] Test each source individually first
- [ ] Verify aggregation collects all results
- [ ] Confirm transaction boundaries
- [ ] Check error isolation works

### Post-Implementation
- [ ] Run all test scenarios
- [ ] Verify counts match exactly
- [ ] Check logs provide debugging info
- [ ] Ensure no regressions

## Dependencies

- Understanding of current Crawlee usage
- Database transaction support
- Access to test RSS feeds

## Risks and Mitigations

- **Risk**: Performance impact of transactions
  - **Mitigation**: Batch inserts, monitor query time

- **Risk**: Memory issues with many articles
  - **Mitigation**: Process in chunks if needed

- **Risk**: Breaking existing single-source jobs
  - **Mitigation**: Test both single and multi-source

## Definition of Done

- [ ] Multi-source jobs save all content reliably
- [ ] Failed sources don't affect others
- [ ] Transactions ensure consistency
- [ ] Status accurately reflects outcome
- [ ] Comprehensive logs for debugging
- [ ] All test scenarios pass